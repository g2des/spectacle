{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import itertools\n",
    "import time\n",
    "import math\n",
    "import logging\n",
    "import random\n",
    "\n",
    "import mxnet as mx\n",
    "import gluonnlp as nlp\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# context = mx.cpu()  # Enable this to run on CPU\n",
    "context = mx.gpu(0)  # Enable this to run on GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "Here we use the Text8 corpus from the [Large Text Compression\n",
    "Benchmark](http://mattmahoney.net/dc/textdata.html) which includes the first\n",
    "100\n",
    "MB of cleaned text from Wikipedia in English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading C:\\Users\\gnsd1\\.mxnet\\datasets\\text8\\text8-6c70299b.zip from https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/dataset/large_text_compression_benchmark/text8-6c70299b.zip...\n",
      "# sentences: 1701\n",
      "# tokens: 10000 ['anarchism', 'originated', 'as', 'a', 'term']\n",
      "# tokens: 10000 ['reciprocity', 'qualitative', 'impairments', 'in', 'communication']\n",
      "# tokens: 10000 ['with', 'the', 'aegis', 'of', 'zeus']\n"
     ]
    }
   ],
   "source": [
    "text8 = nlp.data.Text8()\n",
    "print('# sentences:', len(text8))\n",
    "for sentence in text8[:3]:\n",
    "    print('# tokens:', len(sentence), sentence[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the tokenized data, we first count all tokens and then construct a\n",
    "vocabulary of all tokens that occur at least 5 times in the dataset. The\n",
    "vocabulary contains a one-to-one mapping between tokens and integers (also\n",
    "called indices or idx for short).\n",
    "\n",
    "Furthermore, we can store the frequency count of each\n",
    "token in the vocabulary as we will require this information later on for\n",
    "sampling random negative (or noise) words. Finally, we replace all tokens with\n",
    "their integer representation based on the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# sentences: 1701\n",
      "# tokens: 9895 [5233, 3083, 11, 5, 194]\n",
      "# tokens: 9858 [18214, 17356, 36672, 4, 1753]\n",
      "# tokens: 9926 [23, 0, 19754, 1, 4829]\n"
     ]
    }
   ],
   "source": [
    "counter = nlp.data.count_tokens(itertools.chain.from_iterable(text8))\n",
    "vocab = nlp.Vocab(counter, unknown_token=None, padding_token=None,\n",
    "                  bos_token=None, eos_token=None, min_freq=5)\n",
    "idx_to_counts = [counter[w] for w in vocab.idx_to_token]\n",
    "\n",
    "def code(sentence):\n",
    "    return [vocab[token] for token in sentence if token in vocab]\n",
    "\n",
    "text8 = text8.transform(code, lazy=False)\n",
    "\n",
    "print('# sentences:', len(text8))\n",
    "for sentence in text8[:3]:\n",
    "    print('# tokens:', len(sentence), sentence[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to transform the coded Text8 dataset into batches that are more useful for\n",
    "training an embedding model.\n",
    "\n",
    "In this tutorial we train leveraging the SkipGram\n",
    "objective made popular by the following: [1].\n",
    "\n",
    "For SkipGram, we sample pairs of co-occurring\n",
    "words from the corpus.\n",
    "Two words are said to co-occur if they occur with\n",
    "distance less than a specified *window* size.\n",
    "The *window* size is usually\n",
    "chosen around 5. Refer to the aforementioned paper for more details.\n",
    "\n",
    "To obtain the samples from the corpus, we can shuffle the\n",
    "sentences and then proceed linearly through each sentence, considering each word\n",
    "as well as all the words in its window. In this case, we call the current word\n",
    "in focus the center word, and the words in its window, the context words.\n",
    "GluonNLP contains `gluonnlp.data.EmbeddingCenterContextBatchify` batchify\n",
    "transformation, that takes a corpus, such as the coded Text8 we have here, and\n",
    "returns a `DataStream` of batches of center and context words.\n",
    "\n",
    "To obtain good\n",
    "results, each sentence is further subsampled, meaning that words are deleted\n",
    "with a probability proportional to their frequency.\n",
    "[1] proposes to discard\n",
    "individual occurrences of words from the dataset with probability\n",
    "\n",
    "$$P(w_i) = 1 -\n",
    "\\sqrt{\\frac{t}{f(w_i)}}$$\n",
    "\n",
    "where $f(w_i)$ is the frequency with which a word is\n",
    "observed in a dataset and $t$ is a subsampling constant typically chosen around\n",
    "$10^{-5}$.\n",
    "[1] has also shown that the final performance is improved if the\n",
    "window size is chosen  uniformly random for each center words out of the range\n",
    "[1, *window*].\n",
    "\n",
    "For this notebook, we are interested in training a fastText\n",
    "embedding model [2]. A fastText model not only associates an embedding vector with\n",
    "each token in the vocabulary, but also with a pre-specified number of subwords.\n",
    "Commonly 2 million subword vectors are obtained and each subword vector is\n",
    "associated with zero, one, or multiple character-ngrams. The mapping between\n",
    "character-ngrams and subwords is based on a hash function.\n",
    "The *final* embedding\n",
    "vector of a token is the mean of the vectors associated with the token and all\n",
    "character-ngrams occurring in the string representation of the token. Thereby a\n",
    "fastText embedding model can compute meaningful embedding vectors for tokens\n",
    "that were not seen during training.\n",
    "\n",
    "For this notebook, we have prepared a helper function `transform_data_fasttext`\n",
    "which builds a series of transformations of the `text8 Dataset` created above,\n",
    "applying the techniques we mention briefly above. It returns a `DataStream` over batches as\n",
    "well as a `batchify_fn` function that applied to a batch looks up and includes the\n",
    "fastText subwords associated with the center words. Additionally, it returns the subword\n",
    "function which can be used to obtain the subwords of a given string\n",
    "representation of a token. We will take a closer look at the subword function\n",
    "farther on.\n",
    "\n",
    "You can find the `transform_data_fasttext()` function in `data.py` in the\n",
    "archive that can be downloaded via the `Download` button at the top of this page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import transform_data_fasttext\n",
    "\n",
    "batch_size=4096\n",
    "data = nlp.data.SimpleDataStream([text8])  # input is a stream of datasets, here just 1. Allows scaling to larger corpora that don't fit in memory\n",
    "data, batchify_fn, subword_function = transform_data_fasttext(\n",
    "    data, vocab, idx_to_counts, cbow=False, ngrams=[3,4,5,6], ngram_buckets=100000, batch_size=batch_size, window_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = data.transform(batchify_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the number of subwords is potentially\n",
    "different for every word. Therefore the batchify_fn represents a word with its\n",
    "subwords as a row in a compressed sparse row (CSR) matrix. For more information on CSR matrices click here:\n",
    "https://mxnet.incubator.apache.org/tutorials/sparse/csr.html\n",
    "\n",
    "Separating the batchify_fn from the previous word-pair\n",
    "sampling is useful, as it allows parallelization of the CSR matrix construction over\n",
    "multiple CPU cores for separate batches.\n",
    "\n",
    "## Subwords\n",
    "\n",
    "`GluonNLP` provides the concept of a subword function which maps\n",
    "words to a list of indices representing their subword.\n",
    "Possible subword functions\n",
    "include mapping a word to the sequence of it's characters/bytes or hashes of all\n",
    "its ngrams.\n",
    "\n",
    "FastText models use a hash function to map each ngram of a word to\n",
    "a number in range `[0, num_subwords)`. We include the same hash function.\n",
    "Above\n",
    "`transform_data_fasttext` has also returned a `subword_function` object. Let's try it with\n",
    "a few words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<the>\t[51151, 9726, 48960, 61980, 60934, 16280]\n",
      "<of>\t[97102, 64528, 28930]\n",
      "<and>\t[78080, 35020, 30390, 95046, 19624, 25443]\n"
     ]
    }
   ],
   "source": [
    "idx_to_subwordidxs = subword_function(vocab.idx_to_token)\n",
    "for word, subwords in zip(vocab.idx_to_token[:3], idx_to_subwordidxs[:3]):\n",
    "    print('<'+word+'>', subwords, sep = '\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "Here we define a SkipGram model for training fastText embeddings.\n",
    "For\n",
    "Skip-Gram, the model consists of two independent embedding networks.\n",
    "One for the\n",
    "center words, and one for the context words.\n",
    "For center words, subwords are\n",
    "taken into account while for context words only the token itself is taken into\n",
    "account.\n",
    "\n",
    "GluonNLP provides an `nlp.model.train.FasttextEmbeddingModel` block\n",
    "which defines the fastText style embedding with subword support.\n",
    "It can be used\n",
    "for training, but also supports loading models trained with the original C++\n",
    "fastText library from `.bin` files.\n",
    "After training, vectors for arbitrary words\n",
    "can be looked up via `embedding[['a', 'list', 'of', 'potentially', 'unknown',\n",
    "'words']]` where `embedding` is an `nlp.model.train.FasttextEmbeddingModel`.\n",
    "\n",
    "In\n",
    "the `model.py` script we provide a definition for the fastText model for the\n",
    "SkipGram objective.\n",
    "The model definition is a Gluon HybridBlock, meaning that\n",
    "the complete forward / backward pass are compiled and executed directly in the\n",
    "MXNet backend. Not only does the block include the `FasttextEmbeddingModel` for\n",
    "the center words and a simple embedding matrix for the context words, but it\n",
    "also takes care of sampling a specified number of noise words for each center-\n",
    "context pair. These noise words are called negatives, as the resulting center-\n",
    "negative pair is unlikely to occur in the dataset. The model then must learn\n",
    "which word-pairs are negatives and which ones are real. Thereby it obtains\n",
    "meaningful word and subword vectors for all considered tokens. The negatives are\n",
    "sampled from the smoothed unigram frequency distribution.\n",
    "\n",
    "Let's instantiate and\n",
    "initialize the model. We also create a trainer object for updating the\n",
    "parameters with AdaGrad.\n",
    "Finally we print a summary of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "MXNetError",
     "evalue": "[20:31:17] C:\\Jenkins\\workspace\\mxnet-tag\\mxnet\\src\\ndarray\\ndarray.cc:1285: GPU is not enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMXNetError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-ddf8ee19423b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m embedding = SkipGramNet(\n\u001b[0;32m      8\u001b[0m     vocab.token_to_idx, emsize, batch_size, negatives_weights, subword_function, num_negatives=5, smoothing=0.75)\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0membedding\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minitialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0membedding\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhybridize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgluon\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrainer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membedding\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'adagrad'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.05\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\mxnet\\gluon\\block.py\u001b[0m in \u001b[0;36minitialize\u001b[1;34m(self, init, ctx, verbose, force_reinit)\u001b[0m\n\u001b[0;32m    508\u001b[0m             \u001b[0mWhether\u001b[0m \u001b[0mto\u001b[0m \u001b[0mforce\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0minitialization\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mparameter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0malready\u001b[0m \u001b[0minitialized\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    509\u001b[0m         \"\"\"\n\u001b[1;32m--> 510\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minitialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mforce_reinit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    511\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    512\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mhybridize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactive\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\mxnet\\gluon\\parameter.py\u001b[0m in \u001b[0;36minitialize\u001b[1;34m(self, init, ctx, verbose, force_reinit)\u001b[0m\n\u001b[0;32m    860\u001b[0m             \u001b[0minit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_verbosity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 862\u001b[1;33m             \u001b[0mv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minitialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mforce_reinit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mforce_reinit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    863\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    864\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\mxnet\\gluon\\parameter.py\u001b[0m in \u001b[0;36minitialize\u001b[1;34m(self, init, ctx, default_init, force_reinit)\u001b[0m\n\u001b[0;32m    438\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    439\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_deferred_init\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0minit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdefault_init\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 440\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_finish_deferred_init\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    441\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mreset_ctx\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\mxnet\\gluon\\parameter.py\u001b[0m in \u001b[0;36m_finish_deferred_init\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    332\u001b[0m                     initializer.InitDesc(self.name, {'__init__': init}), data)\n\u001b[0;32m    333\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 334\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_init_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    335\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    336\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_init_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctx_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\mxnet\\gluon\\parameter.py\u001b[0m in \u001b[0;36m_init_impl\u001b[1;34m(self, data, ctx_list)\u001b[0m\n\u001b[0;32m    344\u001b[0m             \u001b[0mdev_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice_id\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    345\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 346\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopyto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mctx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ctx_list\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    347\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_init_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    348\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\mxnet\\gluon\\parameter.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    344\u001b[0m             \u001b[0mdev_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice_id\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    345\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 346\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopyto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mctx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ctx_list\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    347\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_init_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    348\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\mxnet\\ndarray\\ndarray.py\u001b[0m in \u001b[0;36mcopyto\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m   2091\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mContext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2092\u001b[0m             \u001b[0mhret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNDArray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_new_alloc_handle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2093\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0m_internal\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_copyto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2094\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2095\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'copyto does not support type '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\mxnet\\ndarray\\register.py\u001b[0m in \u001b[0;36m_copyto\u001b[1;34m(data, out, name, **kwargs)\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\mxnet\\_ctypes\\ndarray.py\u001b[0m in \u001b[0;36m_imperative_invoke\u001b[1;34m(handle, ndargs, keys, vals, out)\u001b[0m\n\u001b[0;32m     90\u001b[0m         \u001b[0mc_str_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m         \u001b[0mc_str_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mvals\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 92\u001b[1;33m         ctypes.byref(out_stypes)))\n\u001b[0m\u001b[0;32m     93\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0moriginal_output\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\mxnet\\base.py\u001b[0m in \u001b[0;36mcheck_call\u001b[1;34m(ret)\u001b[0m\n\u001b[0;32m    251\u001b[0m     \"\"\"\n\u001b[0;32m    252\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 253\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mMXNetError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpy_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_LIB\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMXGetLastError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    254\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMXNetError\u001b[0m: [20:31:17] C:\\Jenkins\\workspace\\mxnet-tag\\mxnet\\src\\ndarray\\ndarray.cc:1285: GPU is not enabled"
     ]
    }
   ],
   "source": [
    "from model import SG as SkipGramNet\n",
    "\n",
    "emsize = 300\n",
    "num_negatives = 5\n",
    "\n",
    "negatives_weights = mx.nd.array(idx_to_counts)\n",
    "embedding = SkipGramNet(\n",
    "    vocab.token_to_idx, emsize, batch_size, negatives_weights, subword_function, num_negatives=5, smoothing=0.75)\n",
    "embedding.initialize(ctx=context)\n",
    "embedding.hybridize()\n",
    "trainer = mx.gluon.Trainer(embedding.collect_params(), 'adagrad', dict(learning_rate=0.05))\n",
    "\n",
    "print(embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the documentation of the forward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(SkipGramNet.hybrid_forward.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start training, let's examine the quality of our randomly initialized\n",
    "embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_vecs_by_row(x):\n",
    "    return x / (mx.nd.sum(x * x, axis=1) + 1e-10).sqrt().reshape((-1, 1))\n",
    "\n",
    "\n",
    "def get_k_closest_tokens(vocab, embedding, k, word):\n",
    "    word_vec = norm_vecs_by_row(embedding[[word]])\n",
    "    vocab_vecs = norm_vecs_by_row(embedding[vocab.idx_to_token])\n",
    "    dot_prod = mx.nd.dot(vocab_vecs, word_vec.T)\n",
    "    indices = mx.nd.topk(\n",
    "        dot_prod.reshape((len(vocab.idx_to_token), )),\n",
    "        k=k + 1,\n",
    "        ret_typ='indices')\n",
    "    indices = [int(i.asscalar()) for i in indices]\n",
    "    result = [vocab.idx_to_token[i] for i in indices[1:]]\n",
    "    print('closest tokens to \"%s\": %s' % (word, \", \".join(result)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_token = \"vector\"\n",
    "get_k_closest_tokens(vocab, embedding, 10, example_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that in the randomly initialized fastText model the closest tokens to\n",
    "\"vector\" are based on overlapping ngrams.\n",
    "\n",
    "## Training\n",
    "\n",
    "Thanks to the Gluon data pipeline and the HybridBlock handling all\n",
    "complexity, our training code is very simple.\n",
    "We iterate over all batches, move\n",
    "them to the appropriate context (GPU), do forward, backward, and parameter update\n",
    "and finally include some helpful print statements for following the training\n",
    "process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_interval = 500\n",
    "\n",
    "def train_embedding(num_epochs):\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        start_time = time.time()\n",
    "        l_avg = 0\n",
    "        log_wc = 0\n",
    "\n",
    "        print('Beginnign epoch %d and resampling data.' % epoch)\n",
    "        for i, batch in enumerate(batches):\n",
    "            batch = [array.as_in_context(context) for array in batch]\n",
    "            with mx.autograd.record():\n",
    "                l = embedding(*batch)\n",
    "            l.backward()\n",
    "            trainer.step(1)\n",
    "\n",
    "            l_avg += l.mean()\n",
    "            log_wc += l.shape[0]\n",
    "            if i % log_interval == 0:\n",
    "                mx.nd.waitall()\n",
    "                wps = log_wc / (time.time() - start_time)\n",
    "                l_avg = l_avg.asscalar() / log_interval\n",
    "                print('epoch %d, iteration %d, loss %.2f, throughput=%.2fK wps'\n",
    "                      % (epoch, i, l_avg, wps / 1000))\n",
    "                start_time = time.time()\n",
    "                log_wc = 0\n",
    "                l_avg = 0\n",
    "\n",
    "        get_k_closest_tokens(vocab, embedding, 10, example_token)\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginnign epoch 1 and resampling data.\n",
      "epoch 1, iteration 0, loss 0.00, throughput=0.49K wps\n",
      "epoch 1, iteration 500, loss 0.54, throughput=269.18K wps\n",
      "epoch 1, iteration 1000, loss 0.48, throughput=265.59K wps\n",
      "epoch 1, iteration 1500, loss 0.46, throughput=264.65K wps\n",
      "epoch 1, iteration 2000, loss 0.45, throughput=270.81K wps\n",
      "epoch 1, iteration 2500, loss 0.44, throughput=266.88K wps\n",
      "epoch 1, iteration 3000, loss 0.43, throughput=268.99K wps\n",
      "epoch 1, iteration 3500, loss 0.43, throughput=267.53K wps\n",
      "epoch 1, iteration 4000, loss 0.42, throughput=267.00K wps\n",
      "epoch 1, iteration 4500, loss 0.42, throughput=264.03K wps\n",
      "epoch 1, iteration 5000, loss 0.42, throughput=266.56K wps\n",
      "epoch 1, iteration 5500, loss 0.42, throughput=266.27K wps\n",
      "epoch 1, iteration 6000, loss 0.41, throughput=268.20K wps\n",
      "epoch 1, iteration 6500, loss 0.41, throughput=263.14K wps\n",
      "epoch 1, iteration 7000, loss 0.41, throughput=264.48K wps\n",
      "epoch 1, iteration 7500, loss 0.41, throughput=268.16K wps\n",
      "epoch 1, iteration 8000, loss 0.40, throughput=262.41K wps\n",
      "epoch 1, iteration 8500, loss 0.41, throughput=264.99K wps\n",
      "epoch 1, iteration 9000, loss 0.41, throughput=268.57K wps\n",
      "epoch 1, iteration 9500, loss 0.41, throughput=267.74K wps\n",
      "epoch 1, iteration 10000, loss 0.41, throughput=269.03K wps\n",
      "epoch 1, iteration 10500, loss 0.40, throughput=264.34K wps\n",
      "epoch 1, iteration 11000, loss 0.40, throughput=266.57K wps\n",
      "epoch 1, iteration 11500, loss 0.40, throughput=264.46K wps\n",
      "epoch 1, iteration 12000, loss 0.40, throughput=265.64K wps\n",
      "closest tokens to \"vector\": eigenvector, bivector, functor, vectoring, vectors, vectra, automorphisms, polynomial, tensor, symmetric\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_embedding(num_epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Similarity and Relatedness Task\n",
    "\n",
    "Word embeddings should capture the\n",
    "relationship between words in natural language.\n",
    "In the Word Similarity and\n",
    "Relatedness Task, word embeddings are evaluated by comparing word similarity\n",
    "scores computed from a pair of words with human labels for the similarity or\n",
    "relatedness of the pair.\n",
    "\n",
    "`GluonNLP` includes a number of common datasets for\n",
    "the Word Similarity and Relatedness Task. The included datasets are listed in\n",
    "the [API documentation](http://gluon-nlp.mxnet.io/api/data.html#word-embedding-evaluation-datasets). We use several of them in the evaluation example below.\n",
    "We first show a few samples from the WordSim353 dataset, to get an overall\n",
    "feeling of the Dataset structure.\n",
    "\n",
    "## Evaluation\n",
    "\n",
    "Thanks to the subword support of the `FasttextEmbeddingModel` we\n",
    "can evaluate on all words in the evaluation dataset,\n",
    "not only on the ones that we\n",
    "observed during training.\n",
    "\n",
    "We first compute a list of tokens in our evaluation\n",
    "dataset and then create an embedding matrix for them based on the fastText model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading /root/.mxnet/datasets/rarewords/rw.zip from http://www-nlp.stanford.edu/~lmthang/morphoNLM/rw.zip...\n",
      "There are 2951 unique tokens in the RareWords dataset. Examples are:\n",
      "\t ['squishing', 'squirt', 5.88]\n",
      "\t ['undated', 'undatable', 5.83]\n",
      "\t ['circumvents', 'beat', 5.33]\n",
      "\t ['circumvents', 'ebb', 3.25]\n",
      "\t ['dispossess', 'deprive', 6.83]\n",
      "The imputed TokenEmbedding has shape (2951, 300)\n"
     ]
    }
   ],
   "source": [
    "rw = nlp.data.RareWords()\n",
    "rw_tokens  = list(set(itertools.chain.from_iterable((d[0], d[1]) for d in rw)))\n",
    "\n",
    "rw_token_embedding = nlp.embedding.TokenEmbedding(unknown_token=None, allow_extend=True)\n",
    "rw_token_embedding[rw_tokens]= embedding[rw_tokens]\n",
    "\n",
    "print('There are', len(rw_tokens), 'unique tokens in the RareWords dataset. Examples are:')\n",
    "for i in range(5):\n",
    "    print('\\t', rw[i])\n",
    "print('The imputed TokenEmbedding has shape', rw_token_embedding.idx_to_vec.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = nlp.embedding.evaluation.WordEmbeddingSimilarity(\n",
    "    idx_to_vec=rw_token_embedding.idx_to_vec,\n",
    "    similarity_function=\"CosineSimilarity\")\n",
    "evaluator.initialize(ctx=context)\n",
    "evaluator.hybridize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "words1, words2, scores = zip(*([rw_token_embedding.token_to_idx[d[0]],\n",
    "                                rw_token_embedding.token_to_idx[d[1]],\n",
    "                                d[2]] for d in rw))\n",
    "words1 = mx.nd.array(words1, ctx=context)\n",
    "words2 = mx.nd.array(words2, ctx=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman rank correlation on 2034 pairs of RareWords: 0.344\n"
     ]
    }
   ],
   "source": [
    "pred_similarity = evaluator(words1, words2)\n",
    "sr = stats.spearmanr(pred_similarity.asnumpy(), np.array(scores))\n",
    "print('Spearman rank correlation on {} pairs of {}: {}'.format(\n",
    "    len(words1), rw.__class__.__name__, sr.correlation.round(3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further information\n",
    "\n",
    "For further information and examples on training and\n",
    "evaluating word embeddings with GluonNLP take a look at the Word Embedding\n",
    "section on the Scripts / Model Zoo page. There you will find more thorough\n",
    "evaluation techniques and other embedding models. In fact, the `data.py` and\n",
    "`model.py` files used in this example are the same as the ones used in the\n",
    "script.\n",
    "\n",
    "## References\n",
    "\n",
    "- [1] Mikolov, Tomas, et al. “Distributed representations of words and phrases\n",
    "and their compositionally.”\n",
    "   Advances in neural information processing\n",
    "systems. 2013.\n",
    "\n",
    "\n",
    "- [2] Bojanowski et al., \"Enriching Word Vectors with Subword\n",
    "Information\" Transactions of the Association for Computational Linguistics 2017"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
